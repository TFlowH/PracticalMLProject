---
title: "Practical Machine Learning: Course Project\nPredicting How Well A Weight Lifting Activity Is Performed"
author: Rich Seiter
date: Wednesday, June 11, 2014
output: html_document
---

Synopsis
========

```{r init, cache=FALSE}
# Enable caching
# require(knitr)
# opts_chunk$set(cache=TRUE, autodep=TRUE)
# dep_auto() # figure out dependencies automatically

library(caret)
```

Data
====

The data for this report come in the form of CSV training and testing files originally from http://groupware.les.inf.puc-rio.br/har and downloaded on `r readChar("./data/dateDownloaded.txt", 1e5)`.  See download_data.R for details.

The class variable represents performing a Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  Note that this is unordered.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz34LM1QlJ3  
From the paper [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)

```{r dataRead}
trainPath <- "./data/pml-training.csv"
testPath <- "./data/pml-testing.csv"

train <- read.csv(trainPath)
test <- read.csv(testPath)

nrow(train)
nrow(test)

# Class variable
summary(train$classe)

#summary(train)
#summary(test)
```

Data Processing
===============

Note issues with #DIV/0! in computed fields
6 users in train and same 6 in test

Note problem_id field in test

They selected 17 features using M. A. Hall. Correlation-based Feature Subset Selection
for Machine Learning.  Try using this in Weka?

X is a row number.  Remove.
remove timestamps, numwindow

```{r dataProc}
# Check for and remove near zero variance variables
# What is with all the variables having 406 values and 19216 NAs?
# Looks like this has to do with new_window
nzv <- nearZeroVar(train)
nzv
#sapply(nzv, function(x) summary(train[x]))
train <- train[,-nzv]

dataCor <- cor(train[sapply(train, is.numeric)], use="pairwise.complete.obs")
highlyCor <- findCorrelation(dataCor, cutoff = .75)
highlyCor
# require(corrplot) # Nice correlation plot
# corrplot(dataCor)

# combos <- findLinearCombos(train[sapply(train, is.numeric)])
# combos
```

```{r startParallel, cache=FALSE, echo=FALSE, message=F, warning=F}
# Enable parallelism by setting maxCores > 1
# Actual Number of cores used will be lesser of maxCores and number available
# maxCores = 4 # Since my primary machine is quad core
maxCores = 2 # Having trouble with paging

# Start parallel clusters (make sure to close down)
library(parallel)
machineCores <- detectCores()

numCores <- min(maxCores, machineCores) # Allow override in customization

usingParallel <- (numCores > 1)

if (usingParallel) {
  library(doSNOW)  
  cl <- makeCluster(numCores, type = "SOCK")
  # make cluster of cores available
  registerDoSNOW(cl)
}
```

Results
=======

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

Having problems with "Error ... final tuning parameters could not be determined"  See
http://stackoverflow.com/questions/19122617/enet-works-but-not-when-run-via-carettrain  

```{r results1}
# First try a simple random forest in caret
library(caret)
# Note multiclass classification adds some issues
trainMetric = "Accuracy" # Classification default, a poor metric if uneven split
trainSummary <- defaultSummary
fitControl <-
  trainControl(
               #method = "none", # For initial testing, requires 1 model (tuneLength or grid)
               method = "cv",
               number = 10, # 10 is default
               repeats = 1,
               verboseIter = TRUE, # Debug, seems to be proving helpful
               classProbs = TRUE, # Needed for twoClassSummary
               summaryFunction = trainSummary,
               selectionFunction = "best", # default, see ?best
               allowParallel = usingParallel
               )
rfGrid <- expand.grid(.mtry = c(5, 10))
gbmGrid <- expand.grid(.interaction.depth = c(4),
                       .n.trees = c(1:10) * 500,
                       .shrinkage = c(.01))
#rfGrid <- gbmGrid
depVar <- "classe"
indVars <- setdiff(colnames(train), depVar)
# Just pick a subset for now.  Use dput(indVars) to create.
# Weka CfsSubsetEval came up with this list after removing a few, merit 0.234?
# 1,3,4,5,40,55,61,109,114,115,118 : 11
indVars <- c("user_name", "roll_belt", "pitch_belt", "yaw_belt", "magnet_belt_z", "gyros_arm_x", "magnet_arm_x", "gyros_dumbbell_y", "magnet_dumbbell_x", "magnet_dumbbell_y", "pitch_forearm")
# Using 10-fold CV consistently gave the same list minus magnet_dumbbell_x

rfFit <- train(train[,indVars],
               train[,depVar],
               method = "rf", # error, try GBM as a hack
               #method = "gbm",
               metric = trainMetric,
               tuneGrid = rfGrid,
               #tuneLength = 1, # Run once for now  # With parallel turn on optimization
               trControl = fitControl,
               # Following arguments for rf
               #ntrees = 301, # Try combatting overfitting
               importance=TRUE
               )

rfFit

rfImp <- varImp(rfFit)
rfImp

plot(rfFit)

# Look at performance
# getTrainPerf(rfFit) # See above
confusionMatrix(rfFit)
```

```{r predict}
rfPred <- predict(rfFit, newdata=test[,indVars])

# Convert the answers to strings and print
answers = as.character(rfPred)
rbind(test$X, answers)

# Write the answers to individual files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```


Supporting Analysis
===================

```{r analysis}
```

```{r finishParallel, echo=FALSE, cache=FALSE}
# Close clusters after model training complete
if (usingParallel) {
  #Stop parallel cluster
  stopCluster(cl)
  # And make sure caret knows it
  registerDoSEQ()
}
```

Paper Notes
===========

Feature selection discussed on page 3 section 5.1.  
They used random forest (page 4 section 5.2).  10 forests of 10 trees seems like an odd choice.  
Confusion matrix is for the leave one subject out test with overall accuracy 78.2%.  Overall accuracy (of all data, what we should see?) was 98.2%.  
