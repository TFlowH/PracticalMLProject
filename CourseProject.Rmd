---
title: "Practical Machine Learning: Course Project\nPredicting How Well A Weight Lifting Activity Is Performed"
author: Rich Seiter
date: Wednesday, June 11, 2014
output: html_document
---

Synopsis
========

The goal of this project is to use the supplied datasets (accelerometer data tagged with one of five classes of performance, see Data below) to predict the performance class of 20 new observations.  This report outlines an approach using feature selection with cfsSubsetEval in Weka and a cross validated random forest model which achieves over 99% cross validation accuracy and 20/20 correct on the test data.

```{r init, cache=FALSE, message=F, warning=F}
# Enable caching
require(knitr)
opts_chunk$set(cache=TRUE, autodep=TRUE)
dep_auto() # figure out dependencies automatically

library(caret)
```

Data
====

The data for this report come in the form of CSV training and testing files originally from http://groupware.les.inf.puc-rio.br/har and downloaded on `r readChar("./data/dateDownloaded.txt", 1e5)`.  See download_data.R for details.

The class variable represents performing a Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).  Note that this is unordered.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz34LM1QlJ3  
From the paper [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)

```{r dataRead}
trainPath <- "./data/pml-training.csv"
testPath <- "./data/pml-testing.csv"

train <- read.csv(trainPath)
test <- read.csv(testPath)

nrow(train)
nrow(test)

# Class variable
summary(train$classe)

#summary(train)
#summary(test)
```

Data Processing
===============

Rather than using the caret functions nearZeroVar, findCorrelation, and findLinearCombos (or similar techniques) in this section (included for informational purposes) data processing was skipped in favor of doing feature selection in Weka using [CfsSubsetEval](http://wiki.pentaho.com/display/DATAMINING/CfsSubsetEval) starting with the user_name attribute after removing attributes which were "too informative" (like the row number X, timestamps, and numwindow).  This is similar to the technique used by the paper authors referenced as [M. A. Hall. Correlation-based Feature Subset Selection for Machine Learning](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.4643) except they used 17 features rather than 11.  See wekaVars and indVars assignments below for the variables chosen.

Doing feature selection in this fashion avoided a number of issues with the data.  For example, note issues with #DIV/0! in computed fields resulting in numeric values being treated as factors and many attributes containing excessive NAs.

After the initial implementation I became aware of the [FSelector](http://cran.r-project.org/web/packages/FSelector/FSelector.pdf) R package which supports a similar capability (but is less flexible, for example it is not possible to specify a starting set) and is used below as an example of how to do this type of feature selection in R (another alternative would be RWeka).

```{r dataProc}
# Check for and remove near zero variance variables
# What is with all the variables having 406 values and 19216 NAs?
# Looks like this has to do with new_window
nzv <- nearZeroVar(train)
nzv
#sapply(nzv, function(x) summary(train[x]))
train <- train[,-nzv]

dataCor <- cor(train[sapply(train, is.numeric)], use="pairwise.complete.obs")
highlyCor <- findCorrelation(dataCor, cutoff = .75)
highlyCor
# require(corrplot) # Nice correlation plot
# corrplot(dataCor)

# combos <- findLinearCombos(train[sapply(train, is.numeric)])
# combos

# Variable selection using CFS subset
library(FSelector)
useVars <- setdiff(colnames(train), c("X", "raw_timestamp_part_1",
                                      "raw_timestamp_part_2", "cvtd_timestamp",
                                      "num_window"))
cfsVars <- cfs(classe ~ ., train[, useVars])

# Weka CfsSubsetEval came up with this list after removing a few, merit 0.234
# 1,3,4,5,40,55,61,109,114,115,118 : 11
wekaVars <- c("user_name", "roll_belt", "pitch_belt", "yaw_belt",
              "magnet_belt_z", "gyros_arm_x", "magnet_arm_x",
              "gyros_dumbbell_y", "magnet_dumbbell_x", "magnet_dumbbell_y",
              "pitch_forearm")
# Using 10-fold CV for CfsSubsetEval consistently gave the same list minus magnet_dumbbell_x

cfsVars
wekaVars
intersect(wekaVars, cfsVars)
# setdiff(cfsVars, wekaVars)
# setdiff(wekaVars, cfsVars)

indVars <- wekaVars
# indVars <- cfsVars # Try this for comparison
```

The variable lists returned by FSelector and Weka are surprisingly different given they use a similar algorithm.  Given that the Weka implementation was done by M. A. Hall (who wrote the original paper describing the technique) and is more flexible I tend to prefer it.

It turned out FSelector::cfs chose a poor set of variables.  Most of them were almost completely NA!  Therefore I will continue to use the Weka CfsSubsetEval selected variables.

```{r startParallel, cache=FALSE, echo=FALSE, message=F, warning=F}
# Enable parallelism by setting maxCores > 1
# Actual Number of cores used will be lesser of maxCores and number available
# maxCores = 4 # Since my primary machine is quad core
maxCores = 2 # Having trouble with paging

# Start parallel clusters (make sure to close down)
library(parallel)
machineCores <- detectCores()

numCores <- min(maxCores, machineCores) # Allow override in customization

usingParallel <- (numCores > 1)

if (usingParallel) {
  library(doSNOW)  
  cl <- makeCluster(numCores, type = "SOCK")
  # make cluster of cores available
  registerDoSNOW(cl)
}
```


Results
=======

The goal of this project is to predict the manner in which the test subjects did the exercise. This is the "classe" variable in the training set (depVar below).  I chose a random forest model to begin because they usually give high accuracy although there can be runtime (using parallel processing in caret helps with this) and interpretability costs.  The feature selection approach is described above and was chosen based on its use by the authors and my desire to use a technique I recently learned in [More Data Mining with Weka](https://weka.waikato.ac.nz/moredataminingwithweka/course) in a practical setting.

10-fold cross validation was chosen as a good starting model validation technique.  Even though we have enough data to make a train/test split possible, I like the averaging effect of cross validation and have had good experience using it in caret.

For out of sample error I would expect something similar to the observed 99% accuracy for the same subjects (user_name) and 20/20 on the submission lends support to that belief.  But, I would expect worse results for different subjects.  The paper authors did leave one subject out cross validation giving an accuracy of 78% (which seems like a reasonable estimate for new subjects), but I did not feel it was necessary to replicate that technique for this project.

The initial intent was for this to be a baseline model, but given the performance I see no need to change it.

```{r results1}
# First try a simple random forest using caret
library(caret)

# Note multiclass classification adds some issues
trainMetric = "Accuracy" # Classification default, a poor metric if uneven split
trainSummary <- defaultSummary

fitControl <-
  trainControl(
               #method = "none", # For initial testing, requires 1 model (tuneLength or grid)
               method = "cv",
               number = 10, # 10 is default
               repeats = 1,
               verboseIter = TRUE, # Debug, seems to be proving helpful
               classProbs = TRUE, # Needed for twoClassSummary
               summaryFunction = trainSummary,
               selectionFunction = "best", # default, see ?best
               allowParallel = usingParallel
               )

rfGrid <- expand.grid(.mtry = c(5, 10))

depVar <- "classe"
# indVars set in Data Processing above

set.seed(123)
rfFit <- train(train[,indVars],
               train[,depVar],
               method = "rf",
               metric = trainMetric,
               tuneGrid = rfGrid,
               #tuneLength = 1, # Run once for debugging
               trControl = fitControl,
               # Following arguments for rf
               #ntrees = 301, # Try combatting overfitting
               importance=TRUE
               )

# Run time
rfFit$times$everything

rfFit

rfImp <- varImp(rfFit)
rfImp

plot(rfFit)

# Look at performance
confusionMatrix(rfFit)
```

The CV training accuracy is superb at 99%.

The paper authors presented a confusion matrix for the leave one subject out test with overall accuracy 78.2%.  Their overall accuracy (of all data) given was 98.2%.

Now generate predictions on the test set.

```{r predict}
correctAnswers <- structure(c(2L, 1L, 2L, 1L, 1L, 5L, 4L, 2L, 1L, 1L, 2L, 3L, 2L, 
                              1L, 5L, 5L, 1L, 2L, 2L, 2L),
                            .Label = c("A", "B", "C", "D", "E" ),
                            class = "factor")
rfPred <- predict(rfFit, newdata=test[,indVars])

# Convert the answers to strings and print
answers = as.character(rfPred)
rbind(test$X, answers)

# Write the answers to individual files for submission
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("./answers/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```

`r sum(correctAnswers == rfPred)` out of 20 answers correct.

This is the end of the official submission.  Below this is some supplemental analysis which I thought might be of interest, but should not be included in the grading.


Supplemental Analysis
=====================

Decision Tree
-------------

Although accurate, the random forest model above is not very interpretable.  Try a decision tree model here to see if we learn anything more about the data.

```{r tree}
# Since this is a good example of multi class classification try out multiClassSummary
# Use modified multiClassSummary to get as much performance info as possible
source("multiClassSummary.R")
#trainSummary <- multiClassSummary
fitControlMulti <-
  trainControl(method = "cv",
               number = 10, # 10 is default
               repeats = 1,
               verboseIter = TRUE, # Debug, seems to be proving helpful
               classProbs = TRUE, # Needed for twoClassSummary
               #summaryFunction = multiClassSummary,
               summaryFunction = trainSummary,
               selectionFunction = "best", # default, see ?best
               allowParallel = usingParallel
               )

rpartGrid <- expand.grid(.cp = c(0.02, 0.05))

set.seed(123)
rpartFit <- train(train[,indVars],
               train[,depVar],
               method = "rpart",
               metric = trainMetric,
               tuneGrid = rpartGrid,
               #tuneLength = 1, # Run once for debugging
               trControl = fitControl
               # Following arguments for rpart
               )

# Run time
rpartFit$times$everything

rpartFit

plot(rpartFit)

# Look at performance
confusionMatrix(rpartFit)

# Look at the tree itself
library(rattle)
png("CourseProjectTree.png", width=1500, height=800)
fancyRpartPlot(rpartFit$finalModel)
dev.off()
```

The accuracy is clearly inferior to random forest, but for small cp is not that bad.

The tree was too large to display inline, but the PNG is available in the repository.


Leave One Subject Out Cross Validation
--------------------------------------

Mentioned above as being outside of the scope of this project I thought this would be a useful technique to know how to use in the future so decided to try to implement it here.

Henceforth called LOSOCV. (aka LOSOXV)

Most straightforward way to do this appears to be using the index argument to trainControl: http://stats.stackexchange.com/questions/93227/how-to-implement-a-hold-out-validation-in-r  

Did not succeed with this.  Initial work left below in case I want to revisit (code not echoed).

Is this LGOCV in caret?  (see caretTrain.pdf vignette)    
http://stats.stackexchange.com/questions/68472/lgocv-caret-package-r  
It does not look like this is it.

Just use index?  
http://stats.stackexchange.com/questions/93227/how-to-implement-a-hold-out-validation-in-r

Some discussion of [leave-one-subject-out cross-validation](http://stats.stackexchange.com/questions/62823/correct-setup-for-leave-one-subject-out-cross-validation?rq=1)  
[A survey of cross-validation procedures for model selection](http://arxiv.org/abs/0907.4728)  
[LOSOCV at Kaggle](http://www.kaggle.com/c/decoding-the-human-brain/forums/t/8107/clarification-please-public-and-private-split-on-the-leaderboard/44332)

```{r createSubjectFolds, echo=FALSE, eval=FALSE}
# Version of caret::CreateFolds which supports LOSOCV
# Requires

y <- train[,depVar]
subject <- train[,"user_name"]

# Compare to basic createFolds
folds <- createFolds(y)

createSubjectFolds <-
  function (y, subject, list = TRUE, returnTrain = FALSE) 
{
    k <- length(levels(subject))
    if (is.numeric(y)) {
        cuts <- floor(length(y)/k)
        if (cuts < 2) 
            cuts <- 2
        if (cuts > 5) 
            cuts <- 5
        y <- cut(y, unique(quantile(y, probs = seq(0, 1, length = cuts))), 
            include.lowest = TRUE)
    }
    if (k < length(y)) {
        y <- factor(as.character(y))
        numInClass <- table(y)
        foldVector <- vector(mode = "integer", length(y))
        for (i in 1:length(numInClass)) {
            seqVector <- rep(1:k, numInClass[i]%/%k)
            if (numInClass[i]%%k > 0) 
                seqVector <- c(seqVector, sample(1:k, numInClass[i]%%k))
            foldVector[which(y == dimnames(numInClass)$y[i])] <- sample(seqVector)
        }
    }
    else foldVector <- seq(along = y)
    if (list) {
        out <- split(seq(along = y), foldVector)
        names(out) <- paste("Fold", gsub(" ", "0", format(seq(along = out))), 
            sep = "")
        if (returnTrain) 
            out <- lapply(out, function(data, y) y[-data], y = seq(along = y))
    }
    else out <- foldVector
    out
}
```

```{r LOSOCV, echo=T, eval=T}
subject <- train[,"user_name"]
subjectFolds <- length(levels(subject))

subjectIndexes <- list()
for (i in 1:length(levels(subject))){ 
  subjectIndexes[[paste0("Fold",i)]] = which(subject == levels(subject)[i])
}

str(subjectIndexes)
#summary(subjectIndexes)
#lapply(subjectIndexes, summary)

# Compare to basic createFolds
folds <- createFolds(train[,depVar], k=subjectFolds)
# subjectIndexes <- folds # This works fine

fitControlLOSOCV <-
  trainControl(method = "cv",
               number = subjectFolds,
               repeats = 1,
               verboseIter = TRUE, # Debug, seems to be proving helpful
               classProbs = TRUE, # Needed for twoClassSummary
               summaryFunction = trainSummary,
               selectionFunction = "best", # default, see ?best
               index = subjectIndexes,
               allowParallel = usingParallel
               )

rfGridLOSOCV <- expand.grid(.mtry = c(5))

set.seed(123)
rfFitLOSOCV <- train(train[,indVars],
                     train[,depVar],
                     method = "rf",
                     metric = trainMetric,
                     tuneGrid = rfGridLOSOCV,
                     #tuneLength = 1, # Run once for debugging
                     trControl = fitControlLOSOCV,
                     # Following arguments for rf
                     #ntrees = 301, # Try combatting overfitting
                     importance=TRUE
                     )

# Run time
rfFitLOSOCV$times$everything

rfFitLOSOCV

rfImpLOSOCV <- varImp(rfFitLOSOCV)
rfImpLOSOCV

# Look at performance
confusionMatrix(rfFitLOSOCV)
```

It turns out this model is virtually useless for predicting subjects not included in the training set!

Try using a different set of variables (not including user_name) from cfsSubsetEval in Weka.

```{r LOSOCV1, echo=T, eval=T}
# Weka CfsSubsetEval came up with this list after removing a few, merit 0.266
# 3,4,5,61,109,115,118 : 7
# This is a subset of the original set so just try removing user_name from the original
wekaVars1 <- c("roll_belt", "pitch_belt", "yaw_belt",
               "magnet_belt_z", "gyros_arm_x", "magnet_arm_x",
               "gyros_dumbbell_y", "magnet_dumbbell_x", "magnet_dumbbell_y",
               "pitch_forearm")
# Using 10-fold CV for CfsSubsetEval gave similar results

set.seed(123)
rfFitLOSOCV1 <- train(train[,wekaVars1],
                      train[,depVar],
                      method = "rf",
                      metric = trainMetric,
                      tuneGrid = rfGridLOSOCV,
                      #tuneLength = 1, # Run once for debugging
                      trControl = fitControlLOSOCV,
                      # Following arguments for rf
                      #ntrees = 301, # Try combatting overfitting
                      importance=TRUE
                      )

# Run time
rfFitLOSOCV1$times$everything

rfFitLOSOCV1

rfImpLOSOCV1 <- varImp(rfFitLOSOCV1)
rfImpLOSOCV1

# Look at CV performance
confusionMatrix(rfFitLOSOCV1)

# Look at performance of the final model.  This will overfit since it uses the entire
# training set to fit the model.
predLOSOCV1 <- predict(rfFitLOSOCV1)#, train[,wekaVars1])
confusionMatrix(train$classe, predLOSOCV1)
```

This model is still virtually useless for predicting subjects outside of the training set!  Note the difference between the CV accuracy (usually a good estimate for out of sample) and the accuracy on the entire training set (grossly overfit).

I am impressed by the 78.2% LOSOCV accuracy quoted in the paper.  Working to replicate that would be worthwhile, but I have already exceeded my time budget for this project.

```{r finishParallel, echo=FALSE, cache=FALSE}
# Close clusters after model training complete
if (usingParallel) {
  #Stop parallel cluster
  stopCluster(cl)
  # And make sure caret knows it
  registerDoSEQ()
}
```
